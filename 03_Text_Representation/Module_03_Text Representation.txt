========================================
Module ৩ - পার্ট ১: Bag of Words (BoW)
========================================
==> এই মডিউলে আমরা শিখব কীভাবে আমাদের টেক্সট বা শব্দগুলোকে সংখ্যায় বা ভেক্টরে (Vector) রূপান্তর করতে হয়।

Bag of Words (BoW) কী?
নাম শুনেই বোঝা যাচ্ছে—এটা হলো শব্দের একটা "বস্তা"। 
=> ধরো, একটা বস্তায় তুমি অনেকগুলো শব্দ ভরে দিলে। বস্তার ভেতরে শব্দগুলো ওলটপালট হয়ে যায়, তাই না?
কোনটার পর কোনটা ছিল, সেটা আর বোঝা যায় না। শুধু বোঝা যায় কোন শব্দটা আছে আর কয়বার আছে।

=> BoW ঠিক এই কাজটাই করে। সে শুধু গুনতে থাকে কোন শব্দ কয়বার এসেছে। ক্রম বা অর্ডার (Order) তার কাছে গুরুত্বপূর্ণ না।

চলো একটা সহজ উদাহরণ দিয়ে বুঝি -->

ধরো আমাদের কাছে দুটো বাক্য আছে:
Sentence A: "I love NLP"
Sentence B: "I love Python"

ধাপ ১: শব্দভাণ্ডার (Vocabulary) তৈরি করা
প্রথমে আমরা এই দুটি বাক্য থেকে ইউনিক (Unique) শব্দগুলো খুঁজে বের করে একটা লিস্ট বানাব। 
Vocabulary = ["I", "love", "NLP", "Python"]

ধাপ ২: ভেক্টর (Vector) তৈরি করা
এখন আমরা দেখব প্রতিটি বাক্যে এই শব্দগুলো আছে কি নেই (বা কয়বার আছে)।

পরীক্ষা করা:
--------------
Sentence A ("I love NLP"):
=> "I" আছে? হ্যাঁ (১)
=> "love" আছে? হ্যাঁ (১)
=> "NLP" আছে? হ্যাঁ (১)
=> "Python" আছে? না (০) 

অতএব, Vector A: [1, 1, 1, 0]

Sentence B ("I love Python"):
=> "I" আছে? হ্যাঁ (১)
=> "love" আছে? হ্যাঁ (১)
=> "NLP" আছে? না (০)
=> "Python" আছে? হ্যাঁ (১) 
অতএব, Vector B: [1, 1, 0, 1]


Bag of Words (BoW)-এর একটা বড় সমস্যা হলো, সে শব্দের অর্ডার বা ক্রম মনে রাখে না। যেমন:
১. "This is good, not bad."
২. "This is bad, not good."
BoW-এর কাছে এই দুটো বাক্য একদম সমান (কারণ শব্দগুলো একই)। কিন্তু অর্থ তো সম্পূর্ণ উল্টো!

=> এই সমস্যা মেটানোর জন্য আসে N-grams। সে একা একা শব্দ না নিয়ে, পাশের শব্দকে সাথে নিয়ে জুটি বাঁধে।



========================================
Module ৩: Part ২ - N-grams
========================================

N-grams তৈরি করার সময় আমরা একটা জানালা (Window) ধরে এক কদম করে এগোই। লাফ দিয়ে যাই না।
একে বলে Sliding Window।

=> Unigram (N=1): ১টি করে শব্দ। (সাধারণ BoW)
উদাহরণ: "I", "love", "NLP"

=> Bigram (N=2): পাশাপাশি ২টি করে শব্দ।
উদাহরণ: "I love", "love NLP"

=> Trigram (N=3): পাশাপাশি ৩টি করে শব্দ।
উদাহরণ: "I love NLP"

সুবিধা: Bigram বা Trigram ব্যবহার করলে "not good" বা "not bad"-এর মতো ফ্রেজগুলো একসাথে থাকে,
তাই বাক্যের অর্থ বোঝা সহজ হয়।


========================================
Module ৩: Part ৩ - TF-IDF
========================================

BoW-তে শুধু শব্দের সংখ্যা গোনা হয়। কিন্তু সব শব্দের গুরুত্ব কি সমান? যেমন: "The", "is", "a" - এই শব্দগুলো সব 
জায়গায় হাজারবার থাকে। কিন্তু এদের কোনো বিশেষ অর্থ নেই। আবার "Cancer", "Bomb", "Offer" - এই শব্দগুলো কম 
থাকলেও এদের গুরুত্ব অনেক বেশি।

=> TF-IDF ঠিক এই কাজটাই করে। সে গুরুত্বপূর্ণ (Rare) শব্দগুলোকে খুঁজে বের করে এবং ফালতু (Common) শব্দগুলোর 
পাওয়ার কমিয়ে দেয়।

এর দুটো অংশ আছে:

১. TF (Term Frequency): একটা নির্দিষ্ট ডকুমেন্টে শব্দটি কতবার আছে।
সূত্র: (শব্দটি ডকুমেন্টে কতবার আছে) / (ডকুমেন্টে মোট শব্দ সংখ্যা)
সহজ কথা: "এই ফাইলে শব্দটি কত কমন?"

২. IDF (Inverse Document Frequency): শব্দটি পুরো ডেটাসেটে (সব ফাইলে) কতটা রেয়ার বা বিরল।
সহজ কথা: "অন্যান্য ফাইলে কি এই শব্দটি সচরাচর দেখা যায়?" যদি শব্দটি সব ফাইলে থাকে (যেমন "is"), 
তাহলে তার IDF কমে শূন্যের কাছাকাছি হয়ে যায়। আর যদি শব্দটি খুব কম ফাইলে থাকে (যেমন "NASA"), 
তাহলে তার IDF বেড়ে যায়।
-------------------------
ফাইনাল স্কোর: TF × IDF
-------------------------

-------------------------
একটা রিয়েল লাইফ উদাহরণ
-------------------------

ধরো, ক্রিকেট নিয়ে একটা আর্টিকেল আছে। সেখানে "Cricket" শব্দটা ১০ বার আছে, আর "The" শব্দটা ১০০ বার আছে।
=> BoW বলবে: "The" শব্দটা বেশি গুরুত্বপূর্ণ (কারণ ১০০ বার আছে)। [Less Acceptable]
=> TF-IDF বলবে: "The" সব আর্টিকেলেই থাকে, তাই এর গুরুত্ব কম। কিন্তু "Cricket" অন্য আর্টিকেলে (যেমন রান্নার আর্টিকেলে)
থাকে না, তাই এই আর্টিকেলে "Cricket"-এর গুরুত্ব বা স্কোর অনেক বেশি। [More Acceptable]